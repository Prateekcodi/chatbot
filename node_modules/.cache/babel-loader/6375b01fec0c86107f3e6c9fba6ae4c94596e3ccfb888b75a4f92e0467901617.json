{"ast":null,"code":"// API service for frontend to communicate with backend\n// Support both Vite and CRA environment variables\nconst getEnv = (viteKey, craKey) => {\n  // Vite at build-time\n  // @ts-ignore\n  const viteVal = typeof import.meta !== 'undefined' && import.meta.env && import.meta.env[viteKey];\n  // CRA at build-time\n  // @ts-ignore\n  const craVal = typeof process !== 'undefined' && process.env && process.env[craKey];\n  return viteVal || craVal || '';\n};\nconst BACKEND_URL = getEnv('VITE_BACKEND_URL', 'REACT_APP_BACKEND_URL') || 'https://chatbot-1-u7m0.onrender.com';\nexport const sendMessageToGemini = async message => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/ask`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        prompt: message\n      })\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n\n    // Check if we have successful responses from any AI service\n    if (data.responses) {\n      // Find the first successful response (preferably Gemini)\n      const geminiResponse = data.responses.gemini;\n      const cohereResponse = data.responses.cohere;\n      const openrouterResponse = data.responses.openrouter;\n      const glmResponse = data.responses.glm;\n      const deepseekResponse = data.responses.deepseek;\n\n      // Prioritize Gemini, then Cohere, then OpenRouter, then GLM 4.5, then DeepSeek 3.1\n      let aiResponse = null;\n      if (geminiResponse && geminiResponse.success) {\n        aiResponse = geminiResponse;\n      } else if (cohereResponse && cohereResponse.success) {\n        aiResponse = cohereResponse;\n      } else if (openrouterResponse && openrouterResponse.success) {\n        aiResponse = openrouterResponse;\n      } else if (glmResponse && glmResponse.success) {\n        aiResponse = glmResponse;\n      } else if (deepseekResponse && deepseekResponse.success) {\n        aiResponse = deepseekResponse;\n      }\n      if (aiResponse && aiResponse.response) {\n        return {\n          success: true,\n          message: aiResponse.response,\n          data: data\n        };\n      }\n    }\n\n    // If no successful response found\n    return {\n      success: false,\n      message: 'No response from AI',\n      data: data\n    };\n  } catch (error) {\n    console.error('Error calling backend:', error);\n    return {\n      success: false,\n      message: 'Failed to get response from AI. Please try again.',\n      error: error\n    };\n  }\n};\n\n// Streaming API function for real-time typing effect\nexport const sendStreamingMessage = async (message, onChunk, onComplete, onError) => {\n  try {\n    var _response$body;\n    const response = await fetch(`${BACKEND_URL}/api/ask-stream`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        prompt: message\n      })\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const reader = (_response$body = response.body) === null || _response$body === void 0 ? void 0 : _response$body.getReader();\n    if (!reader) {\n      throw new Error('No response body');\n    }\n    const decoder = new TextDecoder();\n    let buffer = '';\n    while (true) {\n      const {\n        done,\n        value\n      } = await reader.read();\n      if (done) break;\n      buffer += decoder.decode(value, {\n        stream: true\n      });\n      const lines = buffer.split('\\n');\n      buffer = lines.pop() || '';\n      for (const line of lines) {\n        if (line.startsWith('data: ')) {\n          try {\n            const data = JSON.parse(line.slice(6));\n            onChunk(data);\n            if (data.type === 'complete') {\n              onComplete(data);\n            }\n          } catch (e) {\n            console.error('Error parsing SSE data:', e);\n          }\n        }\n      }\n    }\n  } catch (error) {\n    onError(error instanceof Error ? error.message : 'Unknown error');\n  }\n};\n\n// Dedicated Chatbot function using the simpler endpoint\nexport const sendChatbotMessage = async message => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/ask`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        prompt: message\n      })\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n\n    // Check if we have successful responses from any AI service\n    if (data.responses) {\n      // Find the first successful response (preferably Gemini)\n      const geminiResponse = data.responses.gemini;\n      const cohereResponse = data.responses.cohere;\n      const openrouterResponse = data.responses.openrouter;\n      const glmResponse = data.responses.glm;\n      const deepseekResponse = data.responses.deepseek;\n\n      // Prioritize Gemini, then Cohere, then OpenRouter, then GLM 4.5, then DeepSeek 3.1\n      let aiResponse = null;\n      if (geminiResponse && geminiResponse.success) {\n        aiResponse = geminiResponse;\n      } else if (cohereResponse && cohereResponse.success) {\n        aiResponse = cohereResponse;\n      } else if (openrouterResponse && openrouterResponse.success) {\n        aiResponse = openrouterResponse;\n      } else if (glmResponse && glmResponse.success) {\n        aiResponse = glmResponse;\n      } else if (deepseekResponse && deepseekResponse.success) {\n        aiResponse = deepseekResponse;\n      }\n      if (aiResponse && aiResponse.response) {\n        return {\n          success: true,\n          message: aiResponse.response,\n          data: data\n        };\n      }\n    }\n\n    // If no successful response found\n    return {\n      success: false,\n      message: 'No response from AI',\n      data: data\n    };\n  } catch (error) {\n    console.error('Error calling chatbot backend:', error);\n    return {\n      success: false,\n      message: 'Failed to get response from AI. Please try again.',\n      error: error\n    };\n  }\n};\n\n// Stream chatbot response via chunked HTTP\nexport async function* streamChatbotMessage(message) {\n  const response = await fetch(`${BACKEND_URL}/api/chatbot-stream`, {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      prompt: message\n    })\n  });\n  if (!response.ok || !response.body) {\n    throw new Error(`Stream error: ${response.status}`);\n  }\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n  try {\n    while (true) {\n      const {\n        done,\n        value\n      } = await reader.read();\n      if (done) break;\n      const chunk = decoder.decode(value, {\n        stream: true\n      });\n      if (chunk) {\n        yield chunk;\n      }\n    }\n  } finally {\n    reader.releaseLock();\n  }\n}\n\n// Get token usage information for all AI services\nexport const getTokenUsage = async () => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/token-usage`, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json'\n      }\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n    return data;\n  } catch (error) {\n    console.error('Error fetching token usage:', error);\n    return {\n      error: 'Failed to fetch token usage',\n      tokenUsage: {}\n    };\n  }\n};\n\n// Get service status for all AI services\nexport const getServiceStatus = async (run = false) => {\n  try {\n    const url = `${BACKEND_URL}/api/service-status${run ? '?run=true' : ''}`;\n    const response = await fetch(url, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json'\n      }\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n    return data;\n  } catch (error) {\n    console.error('Error fetching service status:', error);\n    return {\n      error: 'Failed to fetch service status',\n      services: {},\n      summary: {\n        operational: 0,\n        total: 0,\n        status: 'Service check unavailable'\n      }\n    };\n  }\n};\n\n// Fetch saved conversations from backend (Supabase-backed)\nexport const getConversations = async (page = 1, limit = 20, type) => {\n  try {\n    const params = new URLSearchParams({\n      page: String(page),\n      limit: String(limit)\n    });\n    if (type) params.set('type', type);\n    const response = await fetch(`${BACKEND_URL}/api/conversations?${params.toString()}`, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json'\n      }\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n    return {\n      data: data.data || [],\n      total: data.total || 0\n    };\n  } catch (error) {\n    return {\n      data: [],\n      total: 0\n    };\n  }\n};","map":{"version":3,"names":["getEnv","viteKey","craKey","viteVal","import","meta","env","craVal","process","BACKEND_URL","sendMessageToGemini","message","response","fetch","method","headers","body","JSON","stringify","prompt","ok","Error","status","data","json","responses","geminiResponse","gemini","cohereResponse","cohere","openrouterResponse","openrouter","glmResponse","glm","deepseekResponse","deepseek","aiResponse","success","error","console","sendStreamingMessage","onChunk","onComplete","onError","_response$body","reader","getReader","decoder","TextDecoder","buffer","done","value","read","decode","stream","lines","split","pop","line","startsWith","parse","slice","type","e","sendChatbotMessage","streamChatbotMessage","chunk","releaseLock","getTokenUsage","tokenUsage","getServiceStatus","run","url","services","summary","operational","total","getConversations","page","limit","params","URLSearchParams","String","set","toString"],"sources":["/workspace/src/services/api.ts"],"sourcesContent":["// API service for frontend to communicate with backend\n// Support both Vite and CRA environment variables\nconst getEnv = (viteKey: string, craKey: string) => {\n  // Vite at build-time\n  // @ts-ignore\n  const viteVal = typeof import.meta !== 'undefined' && import.meta.env && import.meta.env[viteKey];\n  // CRA at build-time\n  // @ts-ignore\n  const craVal = typeof process !== 'undefined' && process.env && process.env[craKey];\n  return (viteVal as string) || (craVal as string) || '';\n};\n\nconst BACKEND_URL = getEnv('VITE_BACKEND_URL', 'REACT_APP_BACKEND_URL') || 'https://chatbot-1-u7m0.onrender.com';\n\nexport interface ApiResponse {\n  success: boolean;\n  message: string;\n  data?: any;\n  error?: unknown;\n}\n\nexport const sendMessageToGemini = async (message: string): Promise<ApiResponse> => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/ask`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ prompt: message }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const data = await response.json();\n    \n    // Check if we have successful responses from any AI service\n    if (data.responses) {\n      // Find the first successful response (preferably Gemini)\n      const geminiResponse = data.responses.gemini;\n      const cohereResponse = data.responses.cohere;\n      const openrouterResponse = data.responses.openrouter;\n      const glmResponse = data.responses.glm;\n      const deepseekResponse = data.responses.deepseek;\n      \n      // Prioritize Gemini, then Cohere, then OpenRouter, then GLM 4.5, then DeepSeek 3.1\n      let aiResponse = null;\n      if (geminiResponse && geminiResponse.success) {\n        aiResponse = geminiResponse;\n      } else if (cohereResponse && cohereResponse.success) {\n        aiResponse = cohereResponse;\n      } else if (openrouterResponse && openrouterResponse.success) {\n        aiResponse = openrouterResponse;\n      } else if (glmResponse && glmResponse.success) {\n        aiResponse = glmResponse;\n      } else if (deepseekResponse && deepseekResponse.success) {\n        aiResponse = deepseekResponse;\n      }\n      \n      if (aiResponse && aiResponse.response) {\n        return {\n          success: true,\n          message: aiResponse.response,\n          data: data\n        };\n      }\n    }\n    \n    // If no successful response found\n    return {\n      success: false,\n      message: 'No response from AI',\n      data: data\n    };\n  } catch (error) {\n    console.error('Error calling backend:', error);\n    return {\n      success: false,\n      message: 'Failed to get response from AI. Please try again.',\n      error: error\n    };\n  }\n};\n\n// Streaming API function for real-time typing effect\nexport const sendStreamingMessage = async (\n  message: string,\n  onChunk: (data: any) => void,\n  onComplete: (data: any) => void,\n  onError: (error: string) => void\n): Promise<void> => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/ask-stream`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ prompt: message }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new Error('No response body');\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n\n      buffer += decoder.decode(value, { stream: true });\n      const lines = buffer.split('\\n');\n      buffer = lines.pop() || '';\n\n      for (const line of lines) {\n        if (line.startsWith('data: ')) {\n          try {\n            const data = JSON.parse(line.slice(6));\n            onChunk(data);\n            \n            if (data.type === 'complete') {\n              onComplete(data);\n            }\n          } catch (e) {\n            console.error('Error parsing SSE data:', e);\n          }\n        }\n      }\n    }\n  } catch (error) {\n    onError(error instanceof Error ? error.message : 'Unknown error');\n  }\n};\n\n// Dedicated Chatbot function using the simpler endpoint\nexport const sendChatbotMessage = async (message: string): Promise<ApiResponse> => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/ask`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ prompt: message }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const data = await response.json();\n    \n    // Check if we have successful responses from any AI service\n    if (data.responses) {\n      // Find the first successful response (preferably Gemini)\n      const geminiResponse = data.responses.gemini;\n      const cohereResponse = data.responses.cohere;\n      const openrouterResponse = data.responses.openrouter;\n      const glmResponse = data.responses.glm;\n      const deepseekResponse = data.responses.deepseek;\n      \n      // Prioritize Gemini, then Cohere, then OpenRouter, then GLM 4.5, then DeepSeek 3.1\n      let aiResponse = null;\n      if (geminiResponse && geminiResponse.success) {\n        aiResponse = geminiResponse;\n      } else if (cohereResponse && cohereResponse.success) {\n        aiResponse = cohereResponse;\n      } else if (openrouterResponse && openrouterResponse.success) {\n        aiResponse = openrouterResponse;\n      } else if (glmResponse && glmResponse.success) {\n        aiResponse = glmResponse;\n      } else if (deepseekResponse && deepseekResponse.success) {\n        aiResponse = deepseekResponse;\n      }\n      \n      if (aiResponse && aiResponse.response) {\n        return {\n          success: true,\n          message: aiResponse.response,\n          data: data\n        };\n      }\n    }\n    \n    // If no successful response found\n    return {\n      success: false,\n      message: 'No response from AI',\n      data: data\n    };\n  } catch (error) {\n    console.error('Error calling chatbot backend:', error);\n    return {\n      success: false,\n      message: 'Failed to get response from AI. Please try again.',\n      error: error\n    };\n  }\n};\n\n// Stream chatbot response via chunked HTTP\nexport async function* streamChatbotMessage(message: string): AsyncGenerator<string, void, unknown> {\n  const response = await fetch(`${BACKEND_URL}/api/chatbot-stream`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ prompt: message })\n  });\n\n  if (!response.ok || !response.body) {\n    throw new Error(`Stream error: ${response.status}`);\n  }\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  try {\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n      const chunk = decoder.decode(value, { stream: true });\n      if (chunk) {\n        yield chunk;\n      }\n    }\n  } finally {\n    reader.releaseLock();\n  }\n}\n\n// Get token usage information for all AI services\nexport const getTokenUsage = async (): Promise<any> => {\n  try {\n    const response = await fetch(`${BACKEND_URL}/api/token-usage`, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return data;\n  } catch (error) {\n    console.error('Error fetching token usage:', error);\n    return {\n      error: 'Failed to fetch token usage',\n      tokenUsage: {}\n    };\n  }\n};\n\n// Get service status for all AI services\nexport const getServiceStatus = async (run: boolean = false): Promise<any> => {\n  try {\n    const url = `${BACKEND_URL}/api/service-status${run ? '?run=true' : ''}`;\n    const response = await fetch(url, {\n      method: 'GET',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return data;\n  } catch (error) {\n    console.error('Error fetching service status:', error);\n    return {\n      error: 'Failed to fetch service status',\n      services: {},\n      summary: {\n        operational: 0,\n        total: 0,\n        status: 'Service check unavailable'\n      }\n    };\n  }\n};\n\n// Fetch saved conversations from backend (Supabase-backed)\nexport const getConversations = async (\n  page: number = 1,\n  limit: number = 20,\n  type?: 'multibot' | 'chatbot'\n): Promise<{ data: any[]; total: number }> => {\n  try {\n    const params = new URLSearchParams({ page: String(page), limit: String(limit) });\n    if (type) params.set('type', type);\n    const response = await fetch(`${BACKEND_URL}/api/conversations?${params.toString()}`, {\n      method: 'GET',\n      headers: { 'Content-Type': 'application/json' },\n    });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n    return { data: data.data || [], total: data.total || 0 };\n  } catch (error) {\n    return { data: [], total: 0 };\n  }\n};\n"],"mappings":"AAAA;AACA;AACA,MAAMA,MAAM,GAAGA,CAACC,OAAe,EAAEC,MAAc,KAAK;EAClD;EACA;EACA,MAAMC,OAAO,GAAG,OAAOC,MAAM,CAACC,IAAI,KAAK,WAAW,IAAID,MAAM,CAACC,IAAI,CAACC,GAAG,IAAIF,MAAM,CAACC,IAAI,CAACC,GAAG,CAACL,OAAO,CAAC;EACjG;EACA;EACA,MAAMM,MAAM,GAAG,OAAOC,OAAO,KAAK,WAAW,IAAIA,OAAO,CAACF,GAAG,IAAIE,OAAO,CAACF,GAAG,CAACJ,MAAM,CAAC;EACnF,OAAQC,OAAO,IAAgBI,MAAiB,IAAI,EAAE;AACxD,CAAC;AAED,MAAME,WAAW,GAAGT,MAAM,CAAC,kBAAkB,EAAE,uBAAuB,CAAC,IAAI,qCAAqC;AAShH,OAAO,MAAMU,mBAAmB,GAAG,MAAOC,OAAe,IAA2B;EAClF,IAAI;IACF,MAAMC,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAGJ,WAAW,UAAU,EAAE;MACrDK,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;QAAEC,MAAM,EAAER;MAAQ,CAAC;IAC1C,CAAC,CAAC;IAEF,IAAI,CAACC,QAAQ,CAACQ,EAAE,EAAE;MAChB,MAAM,IAAIC,KAAK,CAAC,uBAAuBT,QAAQ,CAACU,MAAM,EAAE,CAAC;IAC3D;IAEA,MAAMC,IAAI,GAAG,MAAMX,QAAQ,CAACY,IAAI,CAAC,CAAC;;IAElC;IACA,IAAID,IAAI,CAACE,SAAS,EAAE;MAClB;MACA,MAAMC,cAAc,GAAGH,IAAI,CAACE,SAAS,CAACE,MAAM;MAC5C,MAAMC,cAAc,GAAGL,IAAI,CAACE,SAAS,CAACI,MAAM;MAC5C,MAAMC,kBAAkB,GAAGP,IAAI,CAACE,SAAS,CAACM,UAAU;MACpD,MAAMC,WAAW,GAAGT,IAAI,CAACE,SAAS,CAACQ,GAAG;MACtC,MAAMC,gBAAgB,GAAGX,IAAI,CAACE,SAAS,CAACU,QAAQ;;MAEhD;MACA,IAAIC,UAAU,GAAG,IAAI;MACrB,IAAIV,cAAc,IAAIA,cAAc,CAACW,OAAO,EAAE;QAC5CD,UAAU,GAAGV,cAAc;MAC7B,CAAC,MAAM,IAAIE,cAAc,IAAIA,cAAc,CAACS,OAAO,EAAE;QACnDD,UAAU,GAAGR,cAAc;MAC7B,CAAC,MAAM,IAAIE,kBAAkB,IAAIA,kBAAkB,CAACO,OAAO,EAAE;QAC3DD,UAAU,GAAGN,kBAAkB;MACjC,CAAC,MAAM,IAAIE,WAAW,IAAIA,WAAW,CAACK,OAAO,EAAE;QAC7CD,UAAU,GAAGJ,WAAW;MAC1B,CAAC,MAAM,IAAIE,gBAAgB,IAAIA,gBAAgB,CAACG,OAAO,EAAE;QACvDD,UAAU,GAAGF,gBAAgB;MAC/B;MAEA,IAAIE,UAAU,IAAIA,UAAU,CAACxB,QAAQ,EAAE;QACrC,OAAO;UACLyB,OAAO,EAAE,IAAI;UACb1B,OAAO,EAAEyB,UAAU,CAACxB,QAAQ;UAC5BW,IAAI,EAAEA;QACR,CAAC;MACH;IACF;;IAEA;IACA,OAAO;MACLc,OAAO,EAAE,KAAK;MACd1B,OAAO,EAAE,qBAAqB;MAC9BY,IAAI,EAAEA;IACR,CAAC;EACH,CAAC,CAAC,OAAOe,KAAK,EAAE;IACdC,OAAO,CAACD,KAAK,CAAC,wBAAwB,EAAEA,KAAK,CAAC;IAC9C,OAAO;MACLD,OAAO,EAAE,KAAK;MACd1B,OAAO,EAAE,mDAAmD;MAC5D2B,KAAK,EAAEA;IACT,CAAC;EACH;AACF,CAAC;;AAED;AACA,OAAO,MAAME,oBAAoB,GAAG,MAAAA,CAClC7B,OAAe,EACf8B,OAA4B,EAC5BC,UAA+B,EAC/BC,OAAgC,KACd;EAClB,IAAI;IAAA,IAAAC,cAAA;IACF,MAAMhC,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAGJ,WAAW,iBAAiB,EAAE;MAC5DK,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;QAAEC,MAAM,EAAER;MAAQ,CAAC;IAC1C,CAAC,CAAC;IAEF,IAAI,CAACC,QAAQ,CAACQ,EAAE,EAAE;MAChB,MAAM,IAAIC,KAAK,CAAC,uBAAuBT,QAAQ,CAACU,MAAM,EAAE,CAAC;IAC3D;IAEA,MAAMuB,MAAM,IAAAD,cAAA,GAAGhC,QAAQ,CAACI,IAAI,cAAA4B,cAAA,uBAAbA,cAAA,CAAeE,SAAS,CAAC,CAAC;IACzC,IAAI,CAACD,MAAM,EAAE;MACX,MAAM,IAAIxB,KAAK,CAAC,kBAAkB,CAAC;IACrC;IAEA,MAAM0B,OAAO,GAAG,IAAIC,WAAW,CAAC,CAAC;IACjC,IAAIC,MAAM,GAAG,EAAE;IAEf,OAAO,IAAI,EAAE;MACX,MAAM;QAAEC,IAAI;QAAEC;MAAM,CAAC,GAAG,MAAMN,MAAM,CAACO,IAAI,CAAC,CAAC;MAC3C,IAAIF,IAAI,EAAE;MAEVD,MAAM,IAAIF,OAAO,CAACM,MAAM,CAACF,KAAK,EAAE;QAAEG,MAAM,EAAE;MAAK,CAAC,CAAC;MACjD,MAAMC,KAAK,GAAGN,MAAM,CAACO,KAAK,CAAC,IAAI,CAAC;MAChCP,MAAM,GAAGM,KAAK,CAACE,GAAG,CAAC,CAAC,IAAI,EAAE;MAE1B,KAAK,MAAMC,IAAI,IAAIH,KAAK,EAAE;QACxB,IAAIG,IAAI,CAACC,UAAU,CAAC,QAAQ,CAAC,EAAE;UAC7B,IAAI;YACF,MAAMpC,IAAI,GAAGN,IAAI,CAAC2C,KAAK,CAACF,IAAI,CAACG,KAAK,CAAC,CAAC,CAAC,CAAC;YACtCpB,OAAO,CAAClB,IAAI,CAAC;YAEb,IAAIA,IAAI,CAACuC,IAAI,KAAK,UAAU,EAAE;cAC5BpB,UAAU,CAACnB,IAAI,CAAC;YAClB;UACF,CAAC,CAAC,OAAOwC,CAAC,EAAE;YACVxB,OAAO,CAACD,KAAK,CAAC,yBAAyB,EAAEyB,CAAC,CAAC;UAC7C;QACF;MACF;IACF;EACF,CAAC,CAAC,OAAOzB,KAAK,EAAE;IACdK,OAAO,CAACL,KAAK,YAAYjB,KAAK,GAAGiB,KAAK,CAAC3B,OAAO,GAAG,eAAe,CAAC;EACnE;AACF,CAAC;;AAED;AACA,OAAO,MAAMqD,kBAAkB,GAAG,MAAOrD,OAAe,IAA2B;EACjF,IAAI;IACF,MAAMC,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAGJ,WAAW,UAAU,EAAE;MACrDK,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;QAAEC,MAAM,EAAER;MAAQ,CAAC;IAC1C,CAAC,CAAC;IAEF,IAAI,CAACC,QAAQ,CAACQ,EAAE,EAAE;MAChB,MAAM,IAAIC,KAAK,CAAC,uBAAuBT,QAAQ,CAACU,MAAM,EAAE,CAAC;IAC3D;IAEA,MAAMC,IAAI,GAAG,MAAMX,QAAQ,CAACY,IAAI,CAAC,CAAC;;IAElC;IACA,IAAID,IAAI,CAACE,SAAS,EAAE;MAClB;MACA,MAAMC,cAAc,GAAGH,IAAI,CAACE,SAAS,CAACE,MAAM;MAC5C,MAAMC,cAAc,GAAGL,IAAI,CAACE,SAAS,CAACI,MAAM;MAC5C,MAAMC,kBAAkB,GAAGP,IAAI,CAACE,SAAS,CAACM,UAAU;MACpD,MAAMC,WAAW,GAAGT,IAAI,CAACE,SAAS,CAACQ,GAAG;MACtC,MAAMC,gBAAgB,GAAGX,IAAI,CAACE,SAAS,CAACU,QAAQ;;MAEhD;MACA,IAAIC,UAAU,GAAG,IAAI;MACrB,IAAIV,cAAc,IAAIA,cAAc,CAACW,OAAO,EAAE;QAC5CD,UAAU,GAAGV,cAAc;MAC7B,CAAC,MAAM,IAAIE,cAAc,IAAIA,cAAc,CAACS,OAAO,EAAE;QACnDD,UAAU,GAAGR,cAAc;MAC7B,CAAC,MAAM,IAAIE,kBAAkB,IAAIA,kBAAkB,CAACO,OAAO,EAAE;QAC3DD,UAAU,GAAGN,kBAAkB;MACjC,CAAC,MAAM,IAAIE,WAAW,IAAIA,WAAW,CAACK,OAAO,EAAE;QAC7CD,UAAU,GAAGJ,WAAW;MAC1B,CAAC,MAAM,IAAIE,gBAAgB,IAAIA,gBAAgB,CAACG,OAAO,EAAE;QACvDD,UAAU,GAAGF,gBAAgB;MAC/B;MAEA,IAAIE,UAAU,IAAIA,UAAU,CAACxB,QAAQ,EAAE;QACrC,OAAO;UACLyB,OAAO,EAAE,IAAI;UACb1B,OAAO,EAAEyB,UAAU,CAACxB,QAAQ;UAC5BW,IAAI,EAAEA;QACR,CAAC;MACH;IACF;;IAEA;IACA,OAAO;MACLc,OAAO,EAAE,KAAK;MACd1B,OAAO,EAAE,qBAAqB;MAC9BY,IAAI,EAAEA;IACR,CAAC;EACH,CAAC,CAAC,OAAOe,KAAK,EAAE;IACdC,OAAO,CAACD,KAAK,CAAC,gCAAgC,EAAEA,KAAK,CAAC;IACtD,OAAO;MACLD,OAAO,EAAE,KAAK;MACd1B,OAAO,EAAE,mDAAmD;MAC5D2B,KAAK,EAAEA;IACT,CAAC;EACH;AACF,CAAC;;AAED;AACA,OAAO,gBAAgB2B,oBAAoBA,CAACtD,OAAe,EAAyC;EAClG,MAAMC,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAGJ,WAAW,qBAAqB,EAAE;IAChEK,MAAM,EAAE,MAAM;IACdC,OAAO,EAAE;MAAE,cAAc,EAAE;IAAmB,CAAC;IAC/CC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;MAAEC,MAAM,EAAER;IAAQ,CAAC;EAC1C,CAAC,CAAC;EAEF,IAAI,CAACC,QAAQ,CAACQ,EAAE,IAAI,CAACR,QAAQ,CAACI,IAAI,EAAE;IAClC,MAAM,IAAIK,KAAK,CAAC,iBAAiBT,QAAQ,CAACU,MAAM,EAAE,CAAC;EACrD;EAEA,MAAMuB,MAAM,GAAGjC,QAAQ,CAACI,IAAI,CAAC8B,SAAS,CAAC,CAAC;EACxC,MAAMC,OAAO,GAAG,IAAIC,WAAW,CAAC,CAAC;EAEjC,IAAI;IACF,OAAO,IAAI,EAAE;MACX,MAAM;QAAEE,IAAI;QAAEC;MAAM,CAAC,GAAG,MAAMN,MAAM,CAACO,IAAI,CAAC,CAAC;MAC3C,IAAIF,IAAI,EAAE;MACV,MAAMgB,KAAK,GAAGnB,OAAO,CAACM,MAAM,CAACF,KAAK,EAAE;QAAEG,MAAM,EAAE;MAAK,CAAC,CAAC;MACrD,IAAIY,KAAK,EAAE;QACT,MAAMA,KAAK;MACb;IACF;EACF,CAAC,SAAS;IACRrB,MAAM,CAACsB,WAAW,CAAC,CAAC;EACtB;AACF;;AAEA;AACA,OAAO,MAAMC,aAAa,GAAG,MAAAA,CAAA,KAA0B;EACrD,IAAI;IACF,MAAMxD,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAGJ,WAAW,kBAAkB,EAAE;MAC7DK,MAAM,EAAE,KAAK;MACbC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB;IACF,CAAC,CAAC;IAEF,IAAI,CAACH,QAAQ,CAACQ,EAAE,EAAE;MAChB,MAAM,IAAIC,KAAK,CAAC,uBAAuBT,QAAQ,CAACU,MAAM,EAAE,CAAC;IAC3D;IAEA,MAAMC,IAAI,GAAG,MAAMX,QAAQ,CAACY,IAAI,CAAC,CAAC;IAClC,OAAOD,IAAI;EACb,CAAC,CAAC,OAAOe,KAAK,EAAE;IACdC,OAAO,CAACD,KAAK,CAAC,6BAA6B,EAAEA,KAAK,CAAC;IACnD,OAAO;MACLA,KAAK,EAAE,6BAA6B;MACpC+B,UAAU,EAAE,CAAC;IACf,CAAC;EACH;AACF,CAAC;;AAED;AACA,OAAO,MAAMC,gBAAgB,GAAG,MAAAA,CAAOC,GAAY,GAAG,KAAK,KAAmB;EAC5E,IAAI;IACF,MAAMC,GAAG,GAAG,GAAG/D,WAAW,sBAAsB8D,GAAG,GAAG,WAAW,GAAG,EAAE,EAAE;IACxE,MAAM3D,QAAQ,GAAG,MAAMC,KAAK,CAAC2D,GAAG,EAAE;MAChC1D,MAAM,EAAE,KAAK;MACbC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB;IACF,CAAC,CAAC;IAEF,IAAI,CAACH,QAAQ,CAACQ,EAAE,EAAE;MAChB,MAAM,IAAIC,KAAK,CAAC,uBAAuBT,QAAQ,CAACU,MAAM,EAAE,CAAC;IAC3D;IAEA,MAAMC,IAAI,GAAG,MAAMX,QAAQ,CAACY,IAAI,CAAC,CAAC;IAClC,OAAOD,IAAI;EACb,CAAC,CAAC,OAAOe,KAAK,EAAE;IACdC,OAAO,CAACD,KAAK,CAAC,gCAAgC,EAAEA,KAAK,CAAC;IACtD,OAAO;MACLA,KAAK,EAAE,gCAAgC;MACvCmC,QAAQ,EAAE,CAAC,CAAC;MACZC,OAAO,EAAE;QACPC,WAAW,EAAE,CAAC;QACdC,KAAK,EAAE,CAAC;QACRtD,MAAM,EAAE;MACV;IACF,CAAC;EACH;AACF,CAAC;;AAED;AACA,OAAO,MAAMuD,gBAAgB,GAAG,MAAAA,CAC9BC,IAAY,GAAG,CAAC,EAChBC,KAAa,GAAG,EAAE,EAClBjB,IAA6B,KACe;EAC5C,IAAI;IACF,MAAMkB,MAAM,GAAG,IAAIC,eAAe,CAAC;MAAEH,IAAI,EAAEI,MAAM,CAACJ,IAAI,CAAC;MAAEC,KAAK,EAAEG,MAAM,CAACH,KAAK;IAAE,CAAC,CAAC;IAChF,IAAIjB,IAAI,EAAEkB,MAAM,CAACG,GAAG,CAAC,MAAM,EAAErB,IAAI,CAAC;IAClC,MAAMlD,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAGJ,WAAW,sBAAsBuE,MAAM,CAACI,QAAQ,CAAC,CAAC,EAAE,EAAE;MACpFtE,MAAM,EAAE,KAAK;MACbC,OAAO,EAAE;QAAE,cAAc,EAAE;MAAmB;IAChD,CAAC,CAAC;IACF,IAAI,CAACH,QAAQ,CAACQ,EAAE,EAAE;MAChB,MAAM,IAAIC,KAAK,CAAC,uBAAuBT,QAAQ,CAACU,MAAM,EAAE,CAAC;IAC3D;IACA,MAAMC,IAAI,GAAG,MAAMX,QAAQ,CAACY,IAAI,CAAC,CAAC;IAClC,OAAO;MAAED,IAAI,EAAEA,IAAI,CAACA,IAAI,IAAI,EAAE;MAAEqD,KAAK,EAAErD,IAAI,CAACqD,KAAK,IAAI;IAAE,CAAC;EAC1D,CAAC,CAAC,OAAOtC,KAAK,EAAE;IACd,OAAO;MAAEf,IAAI,EAAE,EAAE;MAAEqD,KAAK,EAAE;IAAE,CAAC;EAC/B;AACF,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}